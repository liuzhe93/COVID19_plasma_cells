                :-) GROMACS - gmx mdrun, 2023.4-conda_forge (-:

Executable:   /mnt/zhangzheng_group/liuz-54/miniconda3/envs/py37/bin.AVX2_256/gmx
Data prefix:  /mnt/zhangzheng_group/liuz-54/miniconda3/envs/py37
Working dir:  /mnt/zhangzheng_group/liuz-54/others/MD/3HTB/KIF11
Command line:
  gmx mdrun -deffnm npt

Compiled SIMD is AVX2_256, but CPU also supports AVX_512 (see log).
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Reading file npt.tpr, VERSION 2023.4-conda_forge (single precision)
Changing nstlist from 20 to 80, rlist from 1.222 to 1.319


Update groups can not be used for this system because an incompatible virtual site type is used

Using 96 MPI threads
Using 1 OpenMP thread per tMPI thread

starting mdrun 'Protein in water'
50000 steps,    100.0 ps.

Writing final coordinates.

NOTE: PME load balancing increased the non-bonded workload by more than 50%.
      For better performance, use (more) PME ranks (mdrun -npme),
      or if you are beyond the scaling limit, use fewer total ranks (or nodes).


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 21.2%.
 The balanceable part of the MD step is 27%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 5.8%.
 Average PME mesh/force load: 2.273
 Part of the total run time spent waiting due to PP/PME imbalance: 37.3 %

NOTE: 5.8 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      Dynamic load balancing was automatically disabled, but it might be beneficial to manually turn it on (option -dlb yes.)
      You can also consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.
NOTE: 37.3 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:  2081996.745    21687.557     9600.0
                         6h01:27
                 (ns/day)    (hour/ns)
Performance:        0.398       60.242

GROMACS reminds you: "If Life Seems Jolly Rotten, There's Something You've Forgotten !" (Monty Python)

